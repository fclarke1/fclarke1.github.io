<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-03-11">
<meta name="description" content="What is a Variational AutoEncoder, and how does it differ from a Variational AutoEncoder?">

<title>Fred Clarke - AutoEncoders vs Variational AutoEncoders</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Fred Clarke</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../exploration/index.html"> 
<span class="menu-text">Exploration</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/fclarke1"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/clarkefred/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-an-autoencoder" id="toc-what-is-an-autoencoder" class="nav-link active" data-scroll-target="#what-is-an-autoencoder">What is an AutoEncoder?</a>
  <ul class="collapse">
  <li><a href="#whats-the-purpose-of-an-autoencoder" id="toc-whats-the-purpose-of-an-autoencoder" class="nav-link" data-scroll-target="#whats-the-purpose-of-an-autoencoder">What’s the purpose of an AutoEncoder?</a></li>
  </ul></li>
  <li><a href="#what-is-a-variational-autoencoder" id="toc-what-is-a-variational-autoencoder" class="nav-link" data-scroll-target="#what-is-a-variational-autoencoder">What is a Variational AutoEncoder</a>
  <ul class="collapse">
  <li><a href="#what-loss-is-used-with-a-variational-autoencoder" id="toc-what-loss-is-used-with-a-variational-autoencoder" class="nav-link" data-scroll-target="#what-loss-is-used-with-a-variational-autoencoder">What Loss is used with a Variational AutoEncoder?</a></li>
  </ul></li>
  <li><a href="#autoencoder-vs-variational-autoencoder" id="toc-autoencoder-vs-variational-autoencoder" class="nav-link" data-scroll-target="#autoencoder-vs-variational-autoencoder">AutoEncoder vs Variational AutoEncoder</a>
  <ul class="collapse">
  <li><a href="#performance-as-autoencoders" id="toc-performance-as-autoencoders" class="nav-link" data-scroll-target="#performance-as-autoencoders">Performance as AutoEncoders</a></li>
  <li><a href="#generating-digits" id="toc-generating-digits" class="nav-link" data-scroll-target="#generating-digits">Generating Digits</a></li>
  <li><a href="#latent-space-distribution" id="toc-latent-space-distribution" class="nav-link" data-scroll-target="#latent-space-distribution">Latent Space Distribution</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">AutoEncoders vs Variational AutoEncoders</h1>
  <div class="quarto-categories">
    <div class="quarto-category">machine learning</div>
  </div>
  </div>

<div>
  <div class="description">
    What is a Variational AutoEncoder, and how does it differ from a Variational AutoEncoder?
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">2024-03-11</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><a href="https://github.com/fclarke1/autoencoders-and-vae">Here</a> is my code for any models and images used in this post.</p>
<section id="what-is-an-autoencoder" class="level1">
<h1>What is an AutoEncoder?</h1>
<p>AutoEncoders are a clever way to learn a latent representation of whatever object you have. It’s a model that is trained to output it’s input. This sounds strange but the trick is the model has an information bottleneck by reducing the dimensionality in the middle of the model. This means if the model is able to output a very good representation of the input with the bottleneck, it has learnt a reduced dimensionality representation of the input. The hope is the model has learnt the key features defining the input and it’s discarded less useful information.</p>
<p>Lets see a simple example using the MNIST handwritten digit dataset.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="vae/autoencoder_model.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Autoencoder Model with it’s 2 parts, the encoder maps to the latent space, and the decoder maps from the latent space</figcaption>
</figure>
</div>
<p>Now you can see there are 2 parts to the model. The encoder, which takes the input and reduces the dimensionality down to the latent space (the purple part). And the decoder, which takes the latent space and projects it back to the output dimensionality, and ideally looks similar to the input. There is no restriction on what architecture the encoder and decoder are (CNN, fully connected, RNN, etc.), what defines an autoencoder is an informational bottleneck and it’s output target is the input.</p>
<p>The only loss required for an AutoEncoder is a reconstruction loss, ie. one that is minimising a metric distance between the input and the output. For simplicity in this project I’ll be using the <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html">BinaryCrossEntropy</a></p>
<section id="whats-the-purpose-of-an-autoencoder" class="level2">
<h2 class="anchored" data-anchor-id="whats-the-purpose-of-an-autoencoder">What’s the purpose of an AutoEncoder?</h2>
<p>There are a few benefits of an AutoEncoder:</p>
<ol type="1">
<li><p>Unsupervised Learning In the previous section we never mentioned about the dataset having labels. So this is a way to learn from unlabelled datasets, and then you could transfer learnings from the encoder to another model, eg. image recognition.</p></li>
<li><p>Dimensionality Reduction Having a latent space that has hopefully learnt the key information of an object (eg. handwritten digit images) can be incredibly useful. These representations can then be used in other processes. For example Facebook did something similar when creating a speech representation called <a href="https://ai.meta.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/">wav2vec 2.0</a> which was learnt solely from unlabelled speech audio to learn a latent space. Then the Wav2Vec 2.0 latent space was in a model for Automatic Speech Recognition.</p></li>
<li><p>Generating New Outputs Once a latent space is learnt, you can then take a point from the latent space and feed it into the decoder to generate a new unseen object. <strong>However there are issues doing this</strong>, the latent space is not structured, and therefore there is no gaurentee the generated image has any meaning when decoded. <strong>That’s where the Variational AutoEncoder comes in</strong></p></li>
</ol>
</section>
</section>
<section id="what-is-a-variational-autoencoder" class="level1">
<h1>What is a Variational AutoEncoder</h1>
<p>The benefit of a <a href="https://arxiv.org/pdf/1312.6114.pdf">Variational AutoEncoder</a> is to place restrictions on the latent space to actually define a probability distribution.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="vae/vae_bad_model.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Encoder directly mapping to a probability distribution, then sampling from the distribution causes a back propogation issue</figcaption>
</figure>
</div>
<p>So if we do this directly so that the encoder maps to a probability distributiion and then we sample from that distribution, we are unable to train the model like this because the back propogation has a stochastic element. Instead we do the <strong>reparameterization trick</strong>. To do this the stochastic element is removed from the encoder, instead the encoder maps to parameters of the probability distribution, and we sample the stochastic element from an static distribution, <span class="math inline">\(\epsilon \sim \mathcal{N}(0, I)\)</span>. Therefore we define the latent variable as:</p>
<p><span class="math display">\[
z = \mu + \sigma \odot \epsilon
\]</span></p>
<p>The benefit of this is there is no stochastic back propogation anymore. This becomes clear when we look at the computational graph and show the backpropogation. By sampling from a static probability distribution that isn’t learnt, the gradient backpropogation can extend through the encoders deterministically.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="vae/reparameterization_trick.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Backpropogation is deterministic when using the reparameterisation trick</figcaption>
</figure>
</div>
<p>So now if we put all this together we can look at the model architecture.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="vae/vae_model.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Variational AutoEncoder model architecture where the learnt latent probability space is a Guassian distribution</figcaption>
</figure>
</div>
<p>We are deciding to learn a latent Gaussian distribution space, ie. the encoder outputs are the mean and variance of a Gaussian distribution, which in turn defines how we do the reparameterisation trick. You can choose any distribution but the loss and reparameterisation trick will have to be adjusted.</p>
<section id="what-loss-is-used-with-a-variational-autoencoder" class="level2">
<h2 class="anchored" data-anchor-id="what-loss-is-used-with-a-variational-autoencoder">What Loss is used with a Variational AutoEncoder?</h2>
<p>For a more technical derivation of the VAE and it’s loss see the brilliant <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">Lil’Log blog post on VAEs</a>. For a high level overview we want the learnt probability space parameters the encoder is mapping to, to be the standard Gaussian distribution. To enforce this we use the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler divergence</a> (KL) to minimise the distance between the true posterior <span class="math inline">\(p_{\theta}(z|x)\)</span> and the estimated (and assumed to be gaussian) distribution <span class="math inline">\(q_{\phi}(z|x)\)</span>.</p>
<p>After a bit of expansion and moving terms of <span class="math inline">\(D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))\)</span> we find:</p>
<p><span class="math display">\[
log(p(x_i)) \geq - D_{KL}(q_{\phi}(z|x_i)||p(z)) + E_{\sim q_{\phi}(z|x_i)} \left[ log(p_{\theta}(x_i|z)) \right]
\]</span></p>
<p>Where <span class="math inline">\(x_i\)</span> is a single data point (hand drawn digit). The right handside of the equation is known as the Evidence Lower Bound (ELBO) because maximising it maximises the log-liklihood of of our data. The first <span class="math inline">\(D_{KL}\)</span> is known as the regularizer because it is a constraint on the distribution we are learning <span class="math inline">\(q_{\phi}\)</span>. The second term is known as the reconstruction term because it is a measure on the liklihood of the reconstructed data output. It’s reconstructed because the <span class="math inline">\(p_{\theta}\)</span> probability is conditioned on <span class="math inline">\(z\)</span>.</p>
<p>However we have not explicitly chosen any restrictions on what distributions we are using. As mentioned you can restrict <span class="math inline">\(q\)</span> to be of any type of distribution, but if we choose a multivariate Gaussian with a diagonal covariance then we can create a closed form loss function <span class="math inline">\(L\)</span>. An easy to follow derivation of this can be found in <a href="https://arxiv.org/abs/1907.08956">Odaibo’s derivation</a>, and we get the following loss:</p>
<p><span class="math display">\[
L = - \sum_{j=1}^D \frac{1}{2} [ 1 + log(\sigma_j^2) - \mu_j^2] - \sum_{i=1}^N (||x_i - \hat{x_i}||^2_2)
\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the total number datapoints in your dataset, <span class="math inline">\(D\)</span> is the dimension of your latent space <span class="math inline">\(z\)</span>, and $ is an output from our model from a single sampled <span class="math inline">\(q_{\phi}(z|x_i)\)</span>. During training you can sample multiple <span class="math inline">\(z_i\)</span> to generate multiple <span class="math inline">\(\hat{x_i}\)</span> but with a large enough dataset sampling once is fine to get a good enough estimation of <span class="math inline">\(q_{\phi}(z|x)\)</span>.</p>
</section>
</section>
<section id="autoencoder-vs-variational-autoencoder" class="level1">
<h1>AutoEncoder vs Variational AutoEncoder</h1>
<p>Now we can train up some models and compare the results. <a href="https://github.com/fclarke1/autoencoders-and-vae">Here</a> is code I wrote to train a AutoEncoder and a VAE using the MNIST digit character set. Both models use a fully connected neural net for the encoder and decoder and have a two dimensional latent space. They are the same except the VAE encoder outputs a two dimensional mean <span class="math inline">\(\mu\)</span> and two dimensional <span class="math inline">\(\sigma^2\)</span> (for the diagonal covariance), which used to sample a two dimensional <span class="math inline">\(z\)</span> as input to the decoder.</p>
<section id="performance-as-autoencoders" class="level2">
<h2 class="anchored" data-anchor-id="performance-as-autoencoders">Performance as AutoEncoders</h2>
<p>First, lets have a look at how well both models perform as AutoEncoders. Given an input how close is the output generated from each model? Using the validation set each model achieves a picel-wise accuracy of about 92% after 30 epochs. Visually both models clearly perform well.</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="vae/outputs_auto.png" class="img-fluid" style="width:90.0%"></td>
<td><img src="vae/outputs_vae.png" class="img-fluid" style="width:90.0%"></td>
</tr>
</tbody>
</table>
</section>
<section id="generating-digits" class="level2">
<h2 class="anchored" data-anchor-id="generating-digits">Generating Digits</h2>
<p>Now for the more interesting parts. First we try generating digits using both models. To do this we sample <span class="math inline">\(z\)</span> from a standard 2D Guassian distribution, and then feed <span class="math inline">\(z\)</span> into the model’s decoder to generate an output. We repeat this 81 times to get a 9x9 grid of generated digits from each model.</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>AutoEncoder</th>
<th>VAE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="vae/sample_from_norm_auto.png" class="img-fluid" style="width:90.0%"></td>
<td><img src="vae/sample_from_norm_vae.png" class="img-fluid" style="width:90.0%"></td>
</tr>
</tbody>
</table>
<p>It worked! We are generating hand-written digits. I think that’s pretty cool. Both models seem pretty good, but looking closer I would say the VAE model is a bit better, it’s definietly not perfect but there are less ‘bad’ generated digits.</p>
<p>If we look at the latent space of each model a bit more we should be able to get a view why this is.</p>
</section>
<section id="latent-space-distribution" class="level2">
<h2 class="anchored" data-anchor-id="latent-space-distribution">Latent Space Distribution</h2>
<p>This is the crux of the whole reason we are comparing AutoEncoders to VAEs - the latent space should be more structured compared to the AutoEncoder. The VAE loss ensures this, whereas the AutoEncoder loss only cares about the generated image (from the training set).</p>
<p>Below we are mapping the encoded <span class="math inline">\(z\)</span> 2D values of input digit images for each model, I’ve also added a colour labelling for each point.</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>AutoEncoder</th>
<th>VAE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="vae/latent_dist_auto.png" class="img-fluid" style="width:100.0%"></td>
<td><img src="vae/latent_dist_vae.png" class="img-fluid" style="width:100.0%"></td>
</tr>
</tbody>
</table>
<p>Now we can clearly see the difference. A few points to highlight: - VAE latent distribution is much more like a standard 2d Gaussian distribution (scale, symmetry, density) compared to the AutoEncoder - Using the AutoEncoder to generate images using the standard Gaussian, there is a very low chance of generaing the digit ‘1’. Whereas the VAE has all digits relatively close to [0,0] so there is a decent chance of generating all digits (this can be seen in the ‘Generating Digits’ image)</p>
<p># Thoughts Going through the first principles to create and train a VAE model and compare it to a vanilla AutoEncoer has really illuminated a few things:</p>
<ol type="1">
<li><p>VAE enforces restrictions on the latent space using the ELBO in the loss derivation.</p></li>
<li><p>Any distribution can be used to enforce on the latent distribution, but there is a closed form for the loss using a Gaussian with a diagonal covarience.</p></li>
<li><p>AutoEncoders can be used to generate images but you have to be much more careful how you’re sampling you latent space <span class="math inline">\(z\)</span>. Probably the best way is to perturb a known latent point <span class="math inline">\(z\)</span> you want to mimic.</p></li>
</ol>
<p>Further things I would love to explore on this topic would be:</p>
<ol type="1">
<li><p><a href="https://arxiv.org/pdf/2305.00980.pdf">Conditional Variational AutoEncoders</a> - To give more control on generating outputs from the model you can give the class you want to generate as an input feature. Otherwise, to generate a specific digit with the VAE model I’ve trained, I’d have to find the latent space point of the class I want to generate and perturb that point before inputting it into the decoder. But this doesn’t gaurantee to generate a digit of the same class.</p></li>
<li><p>Using a non-Gaussian distribution in the ELBO derivation for the VAE loss - would using a more complex distribution improve the performance of generating outputs?</p></li>
<li><p>Style Encoding - it would be great to be able to input a reference image (of the same or different class you want to generate) and then generate outputs with a similar style. I’ve seen it <a href="https://google.github.io/tacotron/publications/global_style_tokens/Style%20Tokens%20Unsupervised%20Style%20Modeling%20Control%20and%20Transfer.pdf">here</a> for text-to-speech, but I think it could be applied to this with little change.</p></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/fclarke1\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Fred Clarke</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>
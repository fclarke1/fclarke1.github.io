---
title: AutoEncoders vs Variational AutoEncoders
format:
    html:
        code-fold: true
        code-summary: Code
description: What is a Variational AutoEncoder, and how does it differ from a Variational AutoEncoder?
fig-align: center
categories:
  - machine learning
date-format: iso
date: 2024-03-11
---

This write up is in progress - however the code for this project is completed and there is a [shorter write up with code on my GitHub](https://github.com/fclarke1/autoencoders-and-vae)

# What is an AutoEncoder?
AutoEncoders are a clever way to learn a latent representation of whatever object you have. It's a model that is trained to output it's input. This sounds strange but the trick is the model has an information bottleneck by reducing the dimensionality in the middle of the model. This means if the model is able to output a very good representation of the input with the bottleneck, it has learnt a reduced dimensionality representation of the input. The hope is the model has learnt the key features defining the input and it's discarded less useful information.

Lets see a simple example using the MNIST handwritten digit dataset. 

![Autoencoder Model with it's 2 parts, the encoder maps to the latent space, and the decoder maps from the latent space](vae/autoencoder_model.png){width=80%}

Now you can see there are 2 parts to the model. The encoder, which takes the input and reduces the dimensionality down to the latent space (the purple part). And the decoder, which takes the latent space and projects it back to the output dimensionality, and ideally looks similar to the input. There is no restriction on what architecture the encoder and decoder are (CNN, fully connected, RNN, etc.), what defines an autoencoder is an informational bottleneck and it's output target is the input.

The only loss required for an AutoEncoder is a reconstruction loss, ie. one that is minimising a metric distance between the input and the output. For simplicity in this project I'll be using the [BinaryCrossEntropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) 

## What's the purpose of an AutoEncoder?
There are a few benefits of an AutoEncoder:
1. Unsupervised Learning
In the previous section we never mentioned about the dataset having labels. So this is a way to learn from unlabelled datasets, and then you could transfer learnings from the encoder to another model, eg. image recognition.

2. Dimensionality Reduction
Having a latent space that has hopefully learnt the key information of an object (eg. handwritten digit images) can be incredibly useful. These representations can then be used in other processes. For example Facebook did something similar when creating a speech representation called [wav2vec 2.0](https://ai.meta.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) which was learnt solely from unlabelled speech audio to learn a latent space. Then the Wav2Vec 2.0 latent space was in a model for Automatic Speech Recognition.

3. Generating New Outputs
Once a latent space is learnt, you can then take a point from the latent space and feed it into the decoder to generate a new unseen object. **However there are issues doing this**, the latent space is not structured, and therefore there is no gaurentee the generated image has any meaning when decoded. **That's where the Variational AutoEncoder comes in**


# What is a Variational AutoEncoder
The benefit of a Variational AutoEncoder is to place restrictions on the latent space to actually define a probability distribution. 

![Encoder directly mapping to a probability distribution, then sampling from the distribution causes a back propogation issue](vae/vae_bad_model.png){width=80%}

So if we do this directly so that the encoder maps to a probability distributiion and then we sample from that distribution, we are unable to train the model like this because the back propogation has a stochastic element. Instead we do the **reparameterization trick**. To do this the stochastic element is removed from the encoder, instead the encoder maps to parameters of the probability distribution, and we sample the stochastic element from an static distribution, $\epsilon \sim \mathcal{N}(0, I)$. Therefore we define the latent variable as:

$$
z = \mu + \sigma \odot \epsilon
$$

The benefit of this is there is no stochastic back propogation anymore. This becomes clear when we look at the computational graph and show the backpropogation. By sampling \epsilon from a static probability distribution that isn't learnt, the gradient backpropogation can extend through the encoders deterministically.

![Backpropogation is deterministic when using the reparameterisation trick](vae/reparameterization_trick.png){width=80%}

So now if we put all this together we can look at the model architecture. 

![Variational AutoEncoder model architecture where the learnt latent probability space is a Guassian distribution](vae/vae_model.png){width=80%}

We are deciding to learn a latent Gaussian distribution space, ie. the encoder outputs are the mean and variance of a Gaussian distribution, which in turn defines how we do the reparameterisation trick. You can choose any distribution but the loss and reparameterisation trick will have to be adjusted.


# What Loss is used with a Variational AutoEncoder?
For a more technical derivation of the VAE and it's loss see the brilliant [Lil'Log blog post on VAEs](https://lilianweng.github.io/posts/2018-08-12-vae/). For a high level overview we want the learnt probability space parameters the encoder is mapping to, to be the standard Gaussian distribution. To enforce this we use the [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (KL) to minimise the distance between the true posterior $p_{\theta}(z|x)$ and the estimated (and assumed to be gaussian) distribution $q_{\phi}(z|x)$. 

After a bit of expansion and moving terms of $D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))$ we find:

$$
log(p(x_i)) \geq - D_{KL}(q_{\phi}(z|x_i)||p(z)) + E_{\sim q_{\phi}(z|x_i)} \left[ log(p_{\theta}(x_i|z)) \right]
$$

Where $x_i$ is a single data point (hand drawn digit). The right handside of the equation is known as the Evidence Lower Bound (ELBO) because maximising it maximises the log-liklihood of of our data. The first $D_{KL}$ is known as the regularizer because it is a constraint on the distribution we are learning $q_{\phi}$. The second term is known as the reconstruction term because it is a measure on the liklihood of the reconstructed data output. It's reconstructed because the $p_{\theta}$ probability is conditioned on $z$.

However we have not explicitly chosen any restrictions on what distributions we are using. As mentioned you can restrict $q$ to be of any type of distribution, but if we choose a multivariate Gaussian with a diagonal covariance then we can create a closed form loss function $L$. An easy to follow derivation of this can be found in [Odaibo's derivation](https://arxiv.org/abs/1907.08956), and we get the following loss:

$$
L = - \sum_{j=1}^D \frac{1}{2} [ 1 + log(\sigma_j^2) - \mu_j^2] - \sum_{i=1}^N (||x_i - \hat{x_i}||^2_2)
$$

where $N$ is the total number datapoints in your dataset, $D$ is the dimension of your latent space $z$, and $\hat{x_i} is an output from our model from a single sampled $q_{\phi}(z|x_i)$. During training you can sample multiple $z_i$ to generate multiple \hat{x_i} but with a large enough dataset sampling once is enough to get a good enough estimation of $q_{\phi}(z|x)$.


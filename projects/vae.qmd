---
title: Variational AutoEncoders
format:
    html:
        code-fold: true
        code-summary: Code
description: Generating digits using VAEs
fig-align: center
categories:
  - machine learning
date-format: iso
date: 2024-03-11
---

## What is an AutoEncoder?
AutoEncoders are a clever way to learn a latent representation of whatever it is you have. In our case that's numerical digits.

## How does it work
A machine learning model that tries to predict the input as it's output. So ideally it's learning the identity function. However it's designed with a information bottleneck in the middle. 

The hope is the model will learn the key features of the input and discard less useful information.

## Why use AutoEncoders?
There are few reasons:

1. Unsupervised Learning
Notice that during our training the input itself was the label? This means you can learn from a large dataset without any labels. Then the learnt latent space can be fine-tuned or used for many other things. For example Wav2Vec is a fine-tuned latent space, pre-trained on a large dataset of speech audio followed by fine-tuning for speech recognition on a smaller labelled dataset

2. Generating new objects
they can be the start of generating new data. Take any point in the latent space and put it through the decoder and you'll get a new object. But there's no gaurantee it will look great. That's where VAE comes in
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fred Clarke",
    "section": "",
    "text": "I’m a Data Scientist based in London, recently graduated from UCL with my Masters in Data Science & Machine Learning.\nI love creating little things to understand how things fundamentally work. That’s what this site is for, so I’ve got somewhere to put my little explorations of topics."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Fred Clarke",
    "section": "Education",
    "text": "Education\nUniversity Collage London | 2022 - 2023 MSc Data Science & Machine Learning | Distinction\nUniversity of Warwick | 2011 - 2014 BSc Mathematics | 1st (Hons)"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Fred Clarke",
    "section": "Experience",
    "text": "Experience\nDeepZen | Machine Learning Researcher | Jan 2024 - Present\nDeepZen | Machine Learning Intern | May 2023 - Aug 2023\nMOJU | Data Analytics Engineer | Sep 2022 - Mar 2023\nMOJU | Data and Business Intelligence Manager | Feb 2022 - Sep 2022\nMOJU | Operations Manager | Oct 2019 - Feb 2022\ninnocent | S&OP Manager | Nov 2018 - Oct 2019"
  },
  {
    "objectID": "index.html#fun",
    "href": "index.html#fun",
    "title": "Fred Clarke",
    "section": "Fun",
    "text": "Fun\nRock climbing | Reading | Anything involving the sea (sailing, SUP, swimming)"
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Fred Clarke",
    "section": "Contact Me",
    "text": "Contact Me\nAlways open to hearing from people on any ideas or opportunities:\nfl_clarke ‘at’ hotmail.co.uk"
  },
  {
    "objectID": "exploration/index.html",
    "href": "exploration/index.html",
    "title": "Exploration",
    "section": "",
    "text": "Exploring is fun, and I find the best way to learn. So any little bits I want to understand more, or find interesting I’ll drop in here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an 8-Bit Computer from Scratch\n\n\n\n\n\n\nelectronics\n\n\n\nBuilding an 8-Bit CPU using low-level TTL chips\n\n\n\n\n\nJun 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAutoEncoders vs Variational AutoEncoders\n\n\n\n\n\n\nmachine learning\n\n\n\nWhat is a Variational AutoEncoder, and how does it differ from a Variational AutoEncoder?\n\n\n\n\n\nMar 11, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "exploration/8bit_computer.html",
    "href": "exploration/8bit_computer.html",
    "title": "Building an 8-Bit Computer from Scratch",
    "section": "",
    "text": "Having done a fair bit of programming I eventually asked myself how does a computer actually work and run a program? Initially I thought it would be a quick google, watch a few videos and then that was that. But after at least 100 hours of work, I can confidently say I’m satisfied with the answer I got.\nWhat it comes down to is the CPU, this is the brain of the computer. All programming code, no matter what language, is eventually compiled into machine language the CPU can run a program with. So of course I had to build my own CPU using only low-level TTL chips.\nAfter a bit of a convoluted journey, and an initial failed attempt, I finally have a working 8-bit programmable computer that runs on a custom machine code. I have to say, there were two incredible resources for this:\nHere’s the CPU calculating the Fibonacci sequence using a program detailed at the bottom of this post:"
  },
  {
    "objectID": "exploration/8bit_computer.html#add-x-y",
    "href": "exploration/8bit_computer.html#add-x-y",
    "title": "Building an 8-Bit Computer from Scratch",
    "section": "Add X + Y",
    "text": "Add X + Y\nThe code below is adding 42 + 13. The values 42 and 13 are part of the program and entered on lines 4 and 5. The CPU always starts at line 0 and works sequentially (unless there is a jump). Walking through the program we have:\n\nLoad 42 into register A\nAdd 13 to register A (42) and store answer in register A\noutput register A\nstop the program\n\n\n\n\nLine\nInstruction\nMachine Code\nComment\n\n\n\n\n0\nLDA 4\n0001 0010\nloads value in RAM line 4 into Register A\n\n\n1\nADD 5\n0010 0011\nAdd register A to value in RAM line 5\n\n\n2\nOUT\n1110 xxxx\noutput register A to display\n\n\n3\nHLT\n1111 xxxx\n\n\n\n4\n42\n0010 1010\nenter X value\n\n\n5\n13\n0000 1101\nenter Y value\n\n\n\n\n\n\nThe ALU - where the addition and subtraction take place. Currently showing 10 (register A) + 11 (register B) = 21 (ALU output)"
  },
  {
    "objectID": "exploration/8bit_computer.html#fibonacci-sequence",
    "href": "exploration/8bit_computer.html#fibonacci-sequence",
    "title": "Building an 8-Bit Computer from Scratch",
    "section": "Fibonacci Sequence",
    "text": "Fibonacci Sequence\nThis is a bit of a step up from the one above. It has the conditional JC which jumps if the carry flag is set. The flag is set if the previous add (or SUB, DEC, INC) causes an overflow. So line 6 JC will only trigger when the sequence is above 255 (the largest number the CPU can handle) and will go back to the beginning of the program, otherwise it will skip and go to line 7.\n\n\n\nLine\nInstruction\nMachine Code\nComment\n\n\n\n\n0\nLDI 1\n0101 0001\nstore \\(x_1=1\\)\n\n\n1\nSTA X\n0100 1111\nstore \\(x_2=1\\)\n\n\n2\nSTA Y\n0100 1110\ndummy var needed to swap vars\n\n\n3\nOUT\n1110 xxxx\nprint updated val each loop\n\n\n4\nSTA Y\n0100 1110\n\n\n\n5\nADD X\n0010 1111\nadd previous val\n\n\n6\nJC 0\n1001 0000\nif over 255 restart\n\n\n7\nSWP Y\n1101 1110\nstore previous val\n\n\n8\nSTA X\n0100 1111\nstore previous val\n\n\n9\nLDA Y\n0001 1110\nload current val\n\n\n10\nJMP 3\n1000 0011\nrestart loop\n\n\n11\nX\n\nvar to store previouos val\n\n\n12\nY\n\ndummy var needed to swap vars"
  },
  {
    "objectID": "exploration/8bit_computer.html#factorial",
    "href": "exploration/8bit_computer.html#factorial",
    "title": "Building an 8-Bit Computer from Scratch",
    "section": "Factorial",
    "text": "Factorial\nThis one I’m particularly proud of. It’s really utilising the extension I made with DEC (decrement by 1) to save lines of code, which means I could fit all the code in a measly 16 lines of RAM.\nIn line 15 enter the input to the factorial algorithm (below I’ve entered 4, so we should get 24 as the output). 5 is the max input we can give before it overflows over 255.\nAs the program is shown, I only have one variable I’m using in line 15 (which I called n_fact for making reference easier). But I actually need another 2 variables to keep track of the factorial product, and another which is used for each multiplcation loop (lines 7-&gt;10). I use lines 0 and 1 to store these values, this is ok because the first 2 lines are only run once and then after that I use those lines only to store values. A pretty cool trick that you only get to really see when programming at such a low level.\n\n\n\nLine\nInstruction\nMachine Code\nComment\n\n\n\n\n0\nLDA n_fact\n0001 1111\n\n\n\n1\nSTA 0\n0100 0000\nstore current answer at line 0\n\n\n2\nDEC n_fact\n1100 1111\n\n\n\n3\nDEC n_fact\n1100 1111\ntrack what we need to multiply by each loop\n\n\n4\nJNC 13\n1010 1101\nif n_fact &lt; 0 we are done, jump to end\n\n\n5\nLDA n_fact\n0001 1111\nmultiplicataion loop - RAM(0) * RAM(1)\n\n\n6\nSTA 1\n0100 0001\nmultiplicataion loop\n\n\n7\nLDI 0\n0101 0000\nmultiplicataion loop\n\n\n8\nADD 0\n0010 0000\nmultiplicataion loop\n\n\n9\nDEC 1\n1100 0001\nmultiplicataion loop\n\n\n10\nJC 8\n1001 1000\nmultiplicataion loop\n\n\n11\nSTA 0\n0100 0000\nstore answer\n\n\n12\nJMP 3\n1000 0011\nmult complete, jump up and decrease n_fact\n\n\n13\nOUT\n1110 xxxx\n\n\n\n14\nHLT\n1111 xxxx\n\n\n\n15\nn_fact 4\n0000 0100\nenter factorial input"
  },
  {
    "objectID": "exploration/vae.html",
    "href": "exploration/vae.html",
    "title": "AutoEncoders vs Variational AutoEncoders",
    "section": "",
    "text": "Here is my code for any models and images used in this post."
  },
  {
    "objectID": "exploration/vae.html#whats-the-purpose-of-an-autoencoder",
    "href": "exploration/vae.html#whats-the-purpose-of-an-autoencoder",
    "title": "AutoEncoders vs Variational AutoEncoders",
    "section": "What’s the purpose of an AutoEncoder?",
    "text": "What’s the purpose of an AutoEncoder?\nThere are a few benefits of an AutoEncoder:\n\nUnsupervised Learning In the previous section we never mentioned about the dataset having labels. So this is a way to learn from unlabelled datasets, and then you could transfer learnings from the encoder to another model, eg. image recognition.\nDimensionality Reduction Having a latent space that has hopefully learnt the key information of an object (eg. handwritten digit images) can be incredibly useful. These representations can then be used in other processes. For example Facebook did something similar when creating a speech representation called wav2vec 2.0 which was learnt solely from unlabelled speech audio to learn a latent space. Then the Wav2Vec 2.0 latent space was in a model for Automatic Speech Recognition.\nGenerating New Outputs Once a latent space is learnt, you can then take a point from the latent space and feed it into the decoder to generate a new unseen object. However there are issues doing this, the latent space is not structured, and therefore there is no gaurentee the generated image has any meaning when decoded. That’s where the Variational AutoEncoder comes in"
  },
  {
    "objectID": "exploration/vae.html#what-loss-is-used-with-a-variational-autoencoder",
    "href": "exploration/vae.html#what-loss-is-used-with-a-variational-autoencoder",
    "title": "AutoEncoders vs Variational AutoEncoders",
    "section": "What Loss is used with a Variational AutoEncoder?",
    "text": "What Loss is used with a Variational AutoEncoder?\nFor a more technical derivation of the VAE and it’s loss see the brilliant Lil’Log blog post on VAEs. For a high level overview we want the learnt probability space parameters the encoder is mapping to, to be the standard Gaussian distribution. To enforce this we use the Kullback–Leibler divergence (KL) to minimise the distance between the true posterior \\(p_{\\theta}(z|x)\\) and the estimated (and assumed to be gaussian) distribution \\(q_{\\phi}(z|x)\\).\nAfter a bit of expansion and moving terms of \\(D_{KL}(q_{\\phi}(z|x)||p_{\\theta}(z|x))\\) we find:\n\\[\nlog(p(x_i)) \\geq - D_{KL}(q_{\\phi}(z|x_i)||p(z)) + E_{\\sim q_{\\phi}(z|x_i)} \\left[ log(p_{\\theta}(x_i|z)) \\right]\n\\]\nWhere \\(x_i\\) is a single data point (hand drawn digit). The right handside of the equation is known as the Evidence Lower Bound (ELBO) because maximising it maximises the log-liklihood of of our data. The first \\(D_{KL}\\) is known as the regularizer because it is a constraint on the distribution we are learning \\(q_{\\phi}\\). The second term is known as the reconstruction term because it is a measure on the liklihood of the reconstructed data output. It’s reconstructed because the \\(p_{\\theta}\\) probability is conditioned on \\(z\\).\nHowever we have not explicitly chosen any restrictions on what distributions we are using. As mentioned you can restrict \\(q\\) to be of any type of distribution, but if we choose a multivariate Gaussian with a diagonal covariance then we can create a closed form loss function \\(L\\). An easy to follow derivation of this can be found in Odaibo’s derivation, and we get the following loss:\n\\[\nL = - \\sum_{j=1}^D \\frac{1}{2} [ 1 + log(\\sigma_j^2) - \\mu_j^2] - \\sum_{i=1}^N (||x_i - \\hat{x_i}||^2_2)\n\\]\nwhere \\(N\\) is the total number datapoints in your dataset, \\(D\\) is the dimension of your latent space \\(z\\), and $ is an output from our model from a single sampled \\(q_{\\phi}(z|x_i)\\). During training you can sample multiple \\(z_i\\) to generate multiple \\(\\hat{x_i}\\) but with a large enough dataset sampling once is fine to get a good enough estimation of \\(q_{\\phi}(z|x)\\)."
  },
  {
    "objectID": "exploration/vae.html#performance-as-autoencoders",
    "href": "exploration/vae.html#performance-as-autoencoders",
    "title": "AutoEncoders vs Variational AutoEncoders",
    "section": "Performance as AutoEncoders",
    "text": "Performance as AutoEncoders\nFirst, lets have a look at how well both models perform as AutoEncoders. Given an input how close is the output generated from each model? Using the validation set each model achieves a picel-wise accuracy of about 92% after 30 epochs. Visually both models clearly perform well."
  },
  {
    "objectID": "exploration/vae.html#generating-digits",
    "href": "exploration/vae.html#generating-digits",
    "title": "AutoEncoders vs Variational AutoEncoders",
    "section": "Generating Digits",
    "text": "Generating Digits\nNow for the more interesting parts. First we try generating digits using both models. To do this we sample \\(z\\) from a standard 2D Guassian distribution, and then feed \\(z\\) into the model’s decoder to generate an output. We repeat this 81 times to get a 9x9 grid of generated digits from each model.\n\n\n\n\n\n\n\nAutoEncoder\nVAE\n\n\n\n\n\n\n\n\n\nIt worked! We are generating hand-written digits. I think that’s pretty cool. Both models seem pretty good, but looking closer I would say the VAE model is a bit better, it’s definietly not perfect but there are less ‘bad’ generated digits.\nIf we look at the latent space of each model a bit more we should be able to get a view why this is."
  },
  {
    "objectID": "exploration/vae.html#latent-space-distribution",
    "href": "exploration/vae.html#latent-space-distribution",
    "title": "AutoEncoders vs Variational AutoEncoders",
    "section": "Latent Space Distribution",
    "text": "Latent Space Distribution\nThis is the crux of the whole reason we are comparing AutoEncoders to VAEs - the latent space should be more structured compared to the AutoEncoder. The VAE loss ensures this, whereas the AutoEncoder loss only cares about the generated image (from the training set).\nBelow we are mapping the encoded \\(z\\) 2D values of input digit images for each model, I’ve also added a colour labelling for each point.\n\n\n\n\n\n\n\nAutoEncoder\nVAE\n\n\n\n\n\n\n\n\n\nNow we can clearly see the difference. A few points to highlight: - VAE latent distribution is much more like a standard 2d Gaussian distribution (scale, symmetry, density) compared to the AutoEncoder - Using the AutoEncoder to generate images using the standard Gaussian, there is a very low chance of generaing the digit ‘1’. Whereas the VAE has all digits relatively close to [0,0] so there is a decent chance of generating all digits (this can be seen in the ‘Generating Digits’ image)\n# Thoughts Going through the first principles to create and train a VAE model and compare it to a vanilla AutoEncoer has really illuminated a few things:\n\nVAE enforces restrictions on the latent space using the ELBO in the loss derivation.\nAny distribution can be used to enforce on the latent distribution, but there is a closed form for the loss using a Gaussian with a diagonal covarience.\nAutoEncoders can be used to generate images but you have to be much more careful how you’re sampling you latent space \\(z\\). Probably the best way is to perturb a known latent point \\(z\\) you want to mimic.\n\nFurther things I would love to explore on this topic would be:\n\nConditional Variational AutoEncoders - To give more control on generating outputs from the model you can give the class you want to generate as an input feature. Otherwise, to generate a specific digit with the VAE model I’ve trained, I’d have to find the latent space point of the class I want to generate and perturb that point before inputting it into the decoder. But this doesn’t gaurantee to generate a digit of the same class.\nUsing a non-Gaussian distribution in the ELBO derivation for the VAE loss - would using a more complex distribution improve the performance of generating outputs?\nStyle Encoding - it would be great to be able to input a reference image (of the same or different class you want to generate) and then generate outputs with a similar style. I’ve seen it here for text-to-speech, but I think it could be applied to this with little change."
  }
]